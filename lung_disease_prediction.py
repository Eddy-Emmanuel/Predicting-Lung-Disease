# -*- coding: utf-8 -*-
"""Lung Disease Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pRlhcjArV1fRtWdybC6y6YVzgVm6Y92j
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'lungs-disease-dataset-4-types:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1864500%2F3044546%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240229%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240229T214701Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db7e20275102ad0a180f27953091a5f28f9463a7333b1b160a0dd465901e31f37f41585f4b82147f6fabe8553065c796f50e8676bd150fd7774d2ab57260e0a25fd7960657d42b77ec462463710e103a40a0f56449e8854b2b0796c20af6799abe06e94e89910d652c20479b9b4d02b18644aa46faab6eadcab33c2369ef9f07c93f24f741df781a5cd794958aa9abf34b8bbd7f654b8e54b80d68424ca7f612540188b5c2d0bcc968a5ff3a493134551beb1706091babb98f0d254c934973bc23b248c6152adccaadfc71713714a8bd11ef245bc5001d6be70fd05f04d7e198df1afc9f1784847407b805a7e25bba2b95503c7f3a9b3ff69a1d73dd4a858ce59'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# Commented out IPython magic to ensure Python compatibility.
import os
import cv2
import numpy as np
import pandas as pd
from warnings import filterwarnings
filterwarnings(action="ignore")

import tensorflow as tf
from tensorflow.keras import Input
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import optimizers, callbacks
from tensorflow.keras.applications import efficientnet
from tensorflow.keras.applications.efficientnet import EfficientNetB7
from tensorflow.keras.applications import mobilenet_v2
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing.image import img_to_array, load_img

import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use("ggplot")

data_path = "/kaggle/input/lungs-disease-dataset-4-types/Lung Disease Dataset"

data = {"images":[], "labels":[], "split_type":[]}
for i in os.listdir(data_path): # Data Split
    split_dir = os.path.join(data_path, i)
    for j in os.listdir(split_dir): # Disease
        d_l = os.path.join(split_dir, j)
        for k in os.listdir(d_l):
            data["images"].append(os.path.join(d_l, k))
            data["labels"].append(j)
            data["split_type"].append(i)

df = pd.DataFrame(data)

print(df.shape)

df.head()

df["labels"].value_counts()

df["labels"].value_counts().plot(kind="bar",
                                 title="Target Distribution",
                                 xlabel="Labels",
                                 ylabel="Value counts")
plt.show()

Xtrain, Xtest, ytrain, ytest = train_test_split(df["images"], df["labels"], test_size=0.2, stratify=df["labels"], random_state=42)

xtest, xval, y_test, yval = train_test_split(Xtest, ytest, test_size=0.1, stratify=ytest, random_state=42)

print(Xtrain.shape, xtest.shape, xval.shape)

for i in [Xtrain, ytrain, xtest, xval, y_test, yval]:
    i.reset_index(drop=True, inplace=True)

random_sample = np.random.randint(0, Xtrain.shape[0], 9)

for idx, i in enumerate(random_sample, start=1):
    plt.subplot(3,3,idx)
    load_image = load_img(Xtrain.iloc[i], target_size=(224, 224))
    plt.imshow(load_image)
    plt.title(f"{ytrain.iloc[i]}")
    plt.axis("off")

plt.suptitle("Random Training Sample")
plt.tight_layout()
plt.show()

def Prep_Image(df):
    data = []
    for i in df:
        load_image = load_img(i, target_size=(224, 224))
        data.append(efficientnet.preprocess_input(load_image))
    return np.array(data)

prep_train = Prep_Image(Xtrain)
prep_test = Prep_Image(xtest)
prep_val = Prep_Image(xval)

print(prep_train.shape)
print(prep_test.shape)
print(prep_val.shape)

base_model_1 = EfficientNetB7(include_top=False, weights="imagenet", input_tensor=Input(shape=(224, 224, 3)))

y_train = np.array(to_categorical(LabelEncoder().fit_transform(ytrain)))
y_test_ = np.array(to_categorical(LabelEncoder().fit_transform(y_test)))
y_val = np.array(to_categorical(LabelEncoder().fit_transform(yval)))

print(y_train.shape)
print(y_test_.shape)
print(y_val.shape)

y_train

for layer in  base_model_1.layers:
    layer.trainable = False

base_model_output = base_model_1.output

# x = layers.Conv2D(filters=64, strides=(2, 2), kernel_size=(3, 3), activation="relu")(base_model_output)
x = layers.Dropout(rate=0.5)(base_model_output)
x = layers.BatchNormalization()(x)
x = layers.AveragePooling2D(pool_size=(2, 2))(x)
x = layers.Flatten()(x)
x = layers.Dense(units=512, activation="relu")(x)
x = layers.Dropout(rate=0.5)(x)
x = layers.Dense(units=5, activation="softmax")(x)

model_1 = Model(inputs=base_model_1.input, outputs=x)

model_1.summary()

model_1.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss="categorical_crossentropy", metrics=["accuracy"])

generator = ImageDataGenerator(rotation_range=30,
                               zoom_range=0.2,
                               shear_range=0.2,
                               horizontal_flip=True,
                               fill_mode="nearest")

#  horizontal flip, zoom, shear, rotation, and rescale

model_1_history = model_1.fit(prep_train, y_train, batch_size=32,
                          validation_data=(prep_test, y_test_),
                          epochs=20,
                          callbacks=[callbacks.EarlyStopping(patience=5)])

model_1.evaluate(prep_val, y_val)

pd.DataFrame(model_1_history.history)[["loss", "val_loss"]].plot(xlabel="Epoch", ylabel="Loss")

pd.DataFrame(model_1_history.history)[["accuracy", "val_accuracy"]].plot(xlabel="Epoch", ylabel="Accuracy")

base_model_2 = MobileNetV2(include_top=False, weights="imagenet", input_tensor=Input(shape=(224, 224, 3)))

for layer in  base_model_2.layers:
    layer.trainable = False

base_model_output = base_model_2.output

# x = layers.Conv2D(filters=64, strides=(2, 2), kernel_size=(3, 3), activation="relu")(base_model_output)
x = layers.Dropout(rate=0.5)(base_model_output)
x = layers.BatchNormalization()(x)
x = layers.AveragePooling2D(pool_size=(2, 2))(x)
x = layers.Flatten()(x)
x = layers.Dense(units=512, activation="relu")(x)
x = layers.Dropout(rate=0.5)(x)
x = layers.Dense(units=5, activation="softmax")(x)

model_2 = Model(inputs=base_model_2.input, outputs=x)

model_2.summary()

model_2.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss="categorical_crossentropy", metrics=["accuracy"])

model_2_history = model_2.fit(generator.flow(prep_train, y_train, batch_size=32),
                          validation_data=(prep_test, y_test_),
                          epochs=20,
                          callbacks=[callbacks.EarlyStopping(patience=5)])

pd.DataFrame(model_2_history.history)[["loss", "val_loss"]].plot(xlabel="Epoch", ylabel="Loss")

pd.DataFrame(model_2_history.history)[["accuracy", "val_accuracy"]].plot(xlabel="Epoch", ylabel="Accuracy")